"""
–û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –î–õ–Ø 500K+ –ü–†–û–ú–´–®–õ–ï–ù–ù–´–• –í–ê–ö–ê–ù–°–ò–ô
–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø SQLite (–±–µ–∑ MEDIAN)
"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
from typing import Dict, List, Tuple, Optional, Any
import logging
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

@dataclass
class AnalysisConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö."""
    min_vacancies_for_analysis: int = 1000
    sample_size_large: int = 100000
    chunk_size: int = 50000
    cache_results: bool = True

class IndustrialDataAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è 500K+ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π.
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å SQLite.
    """
    
    def __init__(self, db_path: str = "industrial_vacancies.db"):
        self.db_path = db_path
        self.connection = None
        self.config = AnalysisConfig()
        self.logger = self._setup_logger()
        
        # –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._cache = {}
        
        # –ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.industrial_segments = [
            '–º–∞—à–∏–Ω–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ', '–º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—è', '—Ö–∏–º–∏—á–µ—Å–∫–∞—è', '—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞',
            '–Ω–µ—Ñ—Ç–µ–≥–∞–∑–æ–≤–∞—è', '–≥–æ—Ä–Ω–æ–¥–æ–±—ã–≤–∞—é—â–∞—è', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è', 
            '–ø—Ä–∏–±–æ—Ä–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ', '–¥–µ—Ä–µ–≤–æ–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∞—è', '–ø–∏—â–µ–≤–∞—è'
        ]
        
        # –£—Ä–æ–≤–Ω–∏ –ø–æ–∑–∏—Ü–∏–π
        self.position_levels = [
            '—Ä–∞–±–æ—á–∏–π', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '–∏–Ω–∂–µ–Ω–µ—Ä', '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', '–≤—ã—Å—à–µ–µ_—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ'
        ]

    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        logger = logging.getLogger('IndustrialDataAnalyzer')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger

    def connect_to_database(self) -> bool:
        """
        –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
        """
        try:
            self.connection = sqlite3.connect(self.db_path)
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            self.connection.execute("PRAGMA cache_size = -128000")  # 128MB
            self.connection.execute("PRAGMA temp_store = MEMORY")
            
            self.connection.row_factory = sqlite3.Row
            self.logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return True
            
        except sqlite3.Error as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False

    def get_basic_statistics(self) -> Dict[str, Any]:
        """
        –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–º –≤–∞–∫–∞–Ω—Å–∏—è–º.
        SQLite-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è (–±–µ–∑ MEDIAN).
        """
        cache_key = "basic_statistics"
        if self.config.cache_results and cache_key in self._cache:
            return self._cache[cache_key]
            
        stats = {}
        
        try:
            cursor = self.connection.cursor()
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–±–µ–∑ MEDIAN)
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_vacancies,
                    COUNT(CASE WHEN has_salary = 1 THEN 1 END) as vacancies_with_salary,
                    COUNT(DISTINCT employer_name) as unique_employers,
                    COUNT(DISTINCT region) as unique_regions,
                    AVG(salary_avg_rub) as avg_salary,
                    MIN(salary_avg_rub) as min_salary,
                    MAX(salary_avg_rub) as max_salary
                FROM vacancies 
                WHERE is_industrial = 1
            """)
            
            row = cursor.fetchone()
            if row:
                stats.update(dict(row))
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ–¥–∏–∞–Ω—É –æ—Ç–¥–µ–ª—å–Ω–æ (–∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏)
            cursor.execute("""
                SELECT salary_avg_rub
                FROM vacancies 
                WHERE is_industrial = 1 AND has_salary = 1
                ORDER BY salary_avg_rub
            """)
            salary_data = [row[0] for row in cursor.fetchall() if row[0] is not None]
            
            if salary_data:
                stats['median_salary'] = float(np.median(salary_data))
            else:
                stats['median_salary'] = 0
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞—Ç–∞–º
            cursor.execute("""
                SELECT 
                    MIN(published_at) as earliest_date,
                    MAX(published_at) as latest_date,
                    COUNT(DISTINCT DATE(published_at)) as unique_days
                FROM vacancies 
                WHERE is_industrial = 1
            """)
            
            date_stats = dict(cursor.fetchone())
            stats.update(date_stats)
            
            # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
            if stats.get('total_vacancies', 0) > 0:
                stats['salary_coverage_percent'] = (stats['vacancies_with_salary'] / stats['total_vacancies']) * 100
            else:
                stats['salary_coverage_percent'] = 0
            
            self.logger.info(f"üìä –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ–±—Ä–∞–Ω–∞: {stats.get('total_vacancies', 0):,} –≤–∞–∫–∞–Ω—Å–∏–π")
            
            if self.config.cache_results:
                self._cache[cache_key] = stats
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –±–∞–∑–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            
        return stats

    def analyze_industry_segments_distribution(self) -> Dict[str, Any]:
        """
        –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã–º —Å–µ–≥–º–µ–Ω—Ç–∞–º.
        SQLite-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è.
        """
        cache_key = "industry_segments"
        if self.config.cache_results and cache_key in self._cache:
            return self._cache[cache_key]
            
        analysis = {}
        
        try:
            # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ MEDIAN
            query = """
                SELECT 
                    industry_segment,
                    COUNT(*) as vacancy_count,
                    COUNT(CASE WHEN has_salary = 1 THEN 1 END) as with_salary_count,
                    AVG(salary_avg_rub) as avg_salary,
                    COUNT(DISTINCT employer_name) as unique_employers,
                    COUNT(DISTINCT region) as unique_regions
                FROM vacancies 
                WHERE is_industrial = 1 AND industry_segment IS NOT NULL
                GROUP BY industry_segment
                ORDER BY vacancy_count DESC
            """
            
            df = pd.read_sql_query(query, self.connection)
            
            if not df.empty:
                total_vacancies = df['vacancy_count'].sum()
                df['percentage'] = (df['vacancy_count'] / total_vacancies) * 100
                df['salary_coverage_percent'] = (df['with_salary_count'] / df['vacancy_count']) * 100
                
                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ–¥–∏–∞–Ω—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
                medians = {}
                for segment in df['industry_segment'].unique():
                    median_query = f"""
                        SELECT salary_avg_rub 
                        FROM vacancies 
                        WHERE is_industrial = 1 AND industry_segment = ? AND has_salary = 1
                        ORDER BY salary_avg_rub
                    """
                    cursor = self.connection.cursor()
                    cursor.execute(median_query, (segment,))
                    salaries = [row[0] for row in cursor.fetchall() if row[0] is not None]
                    medians[segment] = float(np.median(salaries)) if salaries else 0
                
                df['median_salary'] = df['industry_segment'].map(medians)
                
                analysis['segments'] = df.to_dict('records')
                analysis['total_segments'] = len(df)
                analysis['dominant_segment'] = df.iloc[0]['industry_segment']
                analysis['dominant_percentage'] = df.iloc[0]['percentage']
                
                # –¢–æ–ø-5 —Å–µ–≥–º–µ–Ω—Ç–æ–≤
                analysis['top_segments'] = df.head(5).to_dict('records')
                
                self.logger.info(f"üè≠ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(df)} –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
                
            if self.config.cache_results:
                self._cache[cache_key] = analysis
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {e}")
            
        return analysis

    def analyze_position_levels_distribution(self) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ —É—Ä–æ–≤–Ω—è–º –ø–æ–∑–∏—Ü–∏–π.
        SQLite-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è.
        """
        cache_key = "position_levels"
        if self.config.cache_results and cache_key in self._cache:
            return self._cache[cache_key]
            
        analysis = {}
        
        try:
            query = """
                SELECT 
                    position_level,
                    COUNT(*) as vacancy_count,
                    AVG(salary_avg_rub) as avg_salary,
                    COUNT(DISTINCT industry_segment) as segments_covered,
                    COUNT(DISTINCT region) as regions_covered
                FROM vacancies 
                WHERE is_industrial = 1 AND position_level IS NOT NULL
                GROUP BY position_level
                ORDER BY vacancy_count DESC
            """
            
            df = pd.read_sql_query(query, self.connection)
            
            if not df.empty:
                total_vacancies = df['vacancy_count'].sum()
                df['percentage'] = (df['vacancy_count'] / total_vacancies) * 100
                
                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ–¥–∏–∞–Ω—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
                medians = {}
                for level in df['position_level'].unique():
                    median_query = f"""
                        SELECT salary_avg_rub 
                        FROM vacancies 
                        WHERE is_industrial = 1 AND position_level = ? AND has_salary = 1
                        ORDER BY salary_avg_rub
                    """
                    cursor = self.connection.cursor()
                    cursor.execute(median_query, (level,))
                    salaries = [row[0] for row in cursor.fetchall() if row[0] is not None]
                    medians[level] = float(np.median(salaries)) if salaries else 0
                
                df['median_salary'] = df['position_level'].map(medians)
                
                analysis['levels'] = df.to_dict('records')
                analysis['most_demanded_level'] = df.iloc[0]['position_level']
                analysis['most_demanded_count'] = df.iloc[0]['vacancy_count']
                
                if not df.empty and 'avg_salary' in df.columns and df['avg_salary'].notna().any():
                    analysis['highest_paid_level'] = df.loc[df['avg_salary'].idxmax()]['position_level']
                    analysis['highest_salary'] = df['avg_salary'].max()
                else:
                    analysis['highest_paid_level'] = '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö'
                    analysis['highest_salary'] = 0
                
                self.logger.info(f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(df)} —É—Ä–æ–≤–Ω–µ–π –ø–æ–∑–∏—Ü–∏–π")
                
            if self.config.cache_results:
                self._cache[cache_key] = analysis
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —É—Ä–æ–≤–Ω–µ–π –ø–æ–∑–∏—Ü–∏–π: {e}")
            
        return analysis

    def analyze_salary_comparison(self) -> Dict[str, Any]:
        """
        –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞—Ä–ø–ª–∞—Ç –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º –∏ —É—Ä–æ–≤–Ω—è–º.
        SQLite-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è.
        """
        cache_key = "salary_comparison"
        if self.config.cache_results and cache_key in self._cache:
            return self._cache[cache_key]
            
        analysis = {}
        
        try:
            # –ó–∞—Ä–ø–ª–∞—Ç—ã –ø–æ —É—Ä–æ–≤–Ω—è–º –ø–æ–∑–∏—Ü–∏–π (–±–µ–∑ STDDEV –∏ MEDIAN)
            query_levels = """
                SELECT 
                    position_level,
                    COUNT(*) as count,
                    AVG(salary_avg_rub) as mean_salary,
                    MIN(salary_avg_rub) as min_salary,
                    MAX(salary_avg_rub) as max_salary
                FROM vacancies 
                WHERE is_industrial = 1 AND has_salary = 1 AND position_level IS NOT NULL
                GROUP BY position_level
                HAVING count >= 10
                ORDER BY mean_salary DESC
            """
            
            df_levels = pd.read_sql_query(query_levels, self.connection)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ–¥–∏–∞–Ω—É –æ—Ç–¥–µ–ª—å–Ω–æ
            medians_levels = {}
            for level in df_levels['position_level'].unique():
                median_query = f"""
                    SELECT salary_avg_rub 
                    FROM vacancies 
                    WHERE is_industrial = 1 AND position_level = ? AND has_salary = 1
                    ORDER BY salary_avg_rub
                """
                cursor = self.connection.cursor()
                cursor.execute(median_query, (level,))
                salaries = [row[0] for row in cursor.fetchall() if row[0] is not None]
                medians_levels[level] = float(np.median(salaries)) if salaries else 0
            
            df_levels['median_salary'] = df_levels['position_level'].map(medians_levels)
            analysis['by_position_level'] = df_levels.to_dict('records')
            
            # –ó–∞—Ä–ø–ª–∞—Ç—ã –ø–æ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã–º —Å–µ–≥–º–µ–Ω—Ç–∞–º
            query_segments = """
                SELECT 
                    industry_segment,
                    COUNT(*) as count,
                    AVG(salary_avg_rub) as mean_salary,
                    MIN(salary_avg_rub) as min_salary,
                    MAX(salary_avg_rub) as max_salary
                FROM vacancies 
                WHERE is_industrial = 1 AND has_salary = 1 AND industry_segment IS NOT NULL
                GROUP BY industry_segment
                HAVING count >= 10
                ORDER BY mean_salary DESC
            """
            
            df_segments = pd.read_sql_query(query_segments, self.connection)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ–¥–∏–∞–Ω—É –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            medians_segments = {}
            for segment in df_segments['industry_segment'].unique():
                median_query = f"""
                    SELECT salary_avg_rub 
                    FROM vacancies 
                    WHERE is_industrial = 1 AND industry_segment = ? AND has_salary = 1
                    ORDER BY salary_avg_rub
                """
                cursor = self.connection.cursor()
                cursor.execute(median_query, (segment,))
                salaries = [row[0] for row in cursor.fetchall() if row[0] is not None]
                medians_segments[segment] = float(np.median(salaries)) if salaries else 0
            
            df_segments['median_salary'] = df_segments['industry_segment'].map(medians_segments)
            analysis['by_industry_segment'] = df_segments.to_dict('records')
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤ vs —Ä–∞–±–æ—á–∏—Ö
            query_comparison = """
                SELECT 
                    position_level,
                    COUNT(*) as count,
                    AVG(salary_avg_rub) as avg_salary,
                    industry_segment
                FROM vacancies 
                WHERE is_industrial = 1 AND has_salary = 1 
                AND position_level IN ('–∏–Ω–∂–µ–Ω–µ—Ä', '—Ä–∞–±–æ—á–∏–π')
                GROUP BY position_level, industry_segment
                HAVING count >= 5
            """
            
            df_comparison = pd.read_sql_query(query_comparison, self.connection)
            
            if not df_comparison.empty:
                engineer_data = df_comparison[df_comparison['position_level'] == '–∏–Ω–∂–µ–Ω–µ—Ä']
                worker_data = df_comparison[df_comparison['position_level'] == '—Ä–∞–±–æ—á–∏–π']
                
                engineer_avg = engineer_data['avg_salary'].mean() if not engineer_data.empty else 0
                worker_avg = worker_data['avg_salary'].mean() if not worker_data.empty else 0
                
                analysis['engineer_vs_worker'] = {
                    'engineer_total_count': engineer_data['count'].sum() if not engineer_data.empty else 0,
                    'engineer_avg_salary': engineer_avg,
                    'worker_total_count': worker_data['count'].sum() if not worker_data.empty else 0,
                    'worker_avg_salary': worker_avg,
                    'salary_ratio': engineer_avg / worker_avg if worker_avg > 0 else 0,
                    'segment_comparison': df_comparison.to_dict('records')
                }
            
            self.logger.info("üí∞ –ê–Ω–∞–ª–∏–∑ –∑–∞—Ä–ø–ª–∞—Ç –∑–∞–≤–µ—Ä—à–µ–Ω")
            
            if self.config.cache_results:
                self._cache[cache_key] = analysis
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞—Ä–ø–ª–∞—Ç: {e}")
            
        return analysis

    def analyze_dynamics(self, period: str = 'monthly') -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–ø—Ä–æ—Å–∞.
        SQLite-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è.
        """
        cache_key = f"dynamics_{period}"
        if self.config.cache_results and cache_key in self._cache:
            return self._cache[cache_key]
            
        analysis = {}
        
        try:
            if period == 'monthly':
                period_format = "strftime('%Y-%m', published_at)"
                period_display = "period || '-01'"
            elif period == 'weekly':
                period_format = "strftime('%Y-%W', published_at)"
                period_display = "period || '-1'"
            else:  # daily
                period_format = "DATE(published_at)"
                period_display = "period"
            
            query = f"""
                SELECT 
                    {period_format} as period,
                    COUNT(*) as vacancy_count,
                    AVG(salary_avg_rub) as avg_salary,
                    COUNT(DISTINCT industry_segment) as segments_active,
                    COUNT(DISTINCT region) as regions_active
                FROM vacancies 
                WHERE is_industrial = 1 AND published_at IS NOT NULL
                GROUP BY period
                ORDER BY period
            """
            
            df = pd.read_sql_query(query, self.connection)
            
            if not df.empty:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø–µ—Ä–∏–æ–¥—ã –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                df['period_date'] = df['period'].astype(str)
                
                df = df.sort_values('period')
                
                # –†–∞—Å—á–µ—Ç —Ç–µ–º–ø–æ–≤ —Ä–æ—Å—Ç–∞
                if len(df) > 1:
                    first_count = df['vacancy_count'].iloc[0]
                    last_count = df['vacancy_count'].iloc[-1]
                    growth_rate = ((last_count - first_count) / first_count) * 100 if first_count > 0 else 0
                    
                    # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
                    df['moving_avg'] = df['vacancy_count'].rolling(window=3, min_periods=1).mean()
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å —Å —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–º–∏ —Ç–∏–ø–∞–º–∏
                    analysis['dynamics'] = []
                    for _, row in df.iterrows():
                        analysis['dynamics'].append({
                            'period': str(row['period']),
                            'vacancy_count': int(row['vacancy_count']),
                            'avg_salary': float(row['avg_salary']) if pd.notna(row['avg_salary']) else 0,
                            'segments_active': int(row['segments_active']),
                            'regions_active': int(row['regions_active']),
                            'moving_avg': float(row['moving_avg']) if pd.notna(row['moving_avg']) else 0
                        })
                    
                    analysis['total_periods'] = len(df)
                    analysis['growth_rate'] = float(growth_rate)
                    analysis['peak_period'] = str(df.loc[df['vacancy_count'].idxmax()]['period'])
                    analysis['peak_count'] = int(df['vacancy_count'].max())
                    
                    # –¢—Ä–µ–Ω–¥ (–ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è)
                    x = np.arange(len(df))
                    y = df['vacancy_count'].values
                    z = np.polyfit(x, y, 1)
                    analysis['trend_slope'] = float(z[0])  # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π = —Ä–æ—Å—Ç, –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π = —Å–ø–∞–¥
                    
                self.logger.info(f"üìà –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–∏–Ω–∞–º–∏–∫–∞ –∑–∞ {len(df)} –ø–µ—Ä–∏–æ–¥–æ–≤")
                
            if self.config.cache_results:
                self._cache[cache_key] = analysis
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–Ω–∞–º–∏–∫–∏: {e}")
            
        return analysis

    def analyze_skills_distribution(self, top_n: int = 20) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –≤–æ—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤.
        –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü—ã skills.
        """
        cache_key = f"skills_{top_n}"
        if self.config.cache_results and cache_key in self._cache:
            return self._cache[cache_key]
            
        analysis = {}
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã skills
            cursor = self.connection.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='skills'")
            if not cursor.fetchone():
                self.logger.warning("‚ö†Ô∏è –¢–∞–±–ª–∏—Ü–∞ skills –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                analysis['top_skills'] = []
                analysis['skill_categories'] = []
                analysis['stats'] = {}
                return analysis
            
            # –¢–æ–ø –Ω–∞–≤—ã–∫–æ–≤ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å)
            query_skills = """
                SELECT 
                    skill_name,
                    skill_category,
                    COUNT(*) as frequency,
                    COUNT(DISTINCT vacancy_id) as unique_vacancies,
                    COUNT(DISTINCT industry_segment) as segments_covered
                FROM skills s
                JOIN vacancies v ON s.vacancy_id = v.id
                WHERE v.is_industrial = 1
                GROUP BY skill_name, skill_category
                ORDER BY frequency DESC
                LIMIT ?
            """
            
            df_skills = pd.read_sql_query(query_skills, self.connection, params=(top_n,))
            
            # –ù–∞–≤—ã–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            query_categories = """
                SELECT 
                    skill_category,
                    COUNT(*) as skill_count,
                    COUNT(DISTINCT skill_name) as unique_skills,
                    COUNT(DISTINCT vacancy_id) as vacancies_with_skills
                FROM skills s
                JOIN vacancies v ON s.vacancy_id = v.id
                WHERE v.is_industrial = 1
                GROUP BY skill_category
                ORDER BY skill_count DESC
            """
            
            df_categories = pd.read_sql_query(query_categories, self.connection)
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞–≤—ã–∫–æ–≤ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å)
            query_stats = """
                SELECT 
                    COUNT(DISTINCT s.skill_name) as total_unique_skills,
                    COUNT(*) as total_skill_mentions,
                    AVG(skills_per_vacancy) as avg_skills_per_vacancy
                FROM (
                    SELECT 
                        vacancy_id,
                        COUNT(*) as skills_per_vacancy
                    FROM skills s
                    JOIN vacancies v ON s.vacancy_id = v.id
                    WHERE v.is_industrial = 1
                    GROUP BY vacancy_id
                ) vacancy_stats,
                skills s
                WHERE s.vacancy_id = vacancy_stats.vacancy_id
            """
            
            try:
                df_stats = pd.read_sql_query(query_stats, self.connection)
                analysis['stats'] = df_stats.iloc[0].to_dict() if not df_stats.empty else {}
            except:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∑–∞–ø—Ä–æ—Å –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
                cursor.execute("SELECT COUNT(DISTINCT skill_name) as total_unique_skills FROM skills")
                unique_skills = cursor.fetchone()[0]
                cursor.execute("SELECT COUNT(*) as total_skill_mentions FROM skills")
                total_mentions = cursor.fetchone()[0]
                analysis['stats'] = {
                    'total_unique_skills': unique_skills,
                    'total_skill_mentions': total_mentions,
                    'avg_skills_per_vacancy': 0
                }
            
            analysis['top_skills'] = df_skills.to_dict('records')
            analysis['skill_categories'] = df_categories.to_dict('records')
            
            self.logger.info(f"üîß –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(df_skills)} —Ç–æ–ø –Ω–∞–≤—ã–∫–æ–≤")
            
            if self.config.cache_results:
                self._cache[cache_key] = analysis
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–≤—ã–∫–æ–≤: {e}")
            analysis['top_skills'] = []
            analysis['skill_categories'] = []
            analysis['stats'] = {}
            
        return analysis

    def analyze_regional_distribution(self) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–π.
        SQLite-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è.
        """
        cache_key = "regional_distribution"
        if self.config.cache_results and cache_key in self._cache:
            return self._cache[cache_key]
            
        analysis = {}
        
        try:
            query = """
                SELECT 
                    region,
                    COUNT(*) as vacancy_count,
                    COUNT(CASE WHEN has_salary = 1 THEN 1 END) as with_salary_count,
                    AVG(salary_avg_rub) as avg_salary,
                    COUNT(DISTINCT industry_segment) as segments_present,
                    COUNT(DISTINCT employer_name) as unique_employers
                FROM vacancies 
                WHERE is_industrial = 1 AND region IS NOT NULL
                GROUP BY region
                HAVING vacancy_count >= 10
                ORDER BY vacancy_count DESC
            """
            
            df = pd.read_sql_query(query, self.connection)
            
            if not df.empty:
                total_vacancies = df['vacancy_count'].sum()
                df['percentage'] = (df['vacancy_count'] / total_vacancies) * 100
                df['salary_coverage_percent'] = (df['with_salary_count'] / df['vacancy_count']) * 100
                
                analysis['regions'] = df.to_dict('records')
                analysis['top_regions'] = df.head(10).to_dict('records')
                analysis['total_regions'] = len(df)
                analysis['dominant_region'] = df.iloc[0]['region']
                analysis['dominant_region_percentage'] = df.iloc[0]['percentage']
                
                if not df.empty and 'avg_salary' in df.columns and df['avg_salary'].notna().any():
                    analysis['highest_paid_region'] = df.loc[df['avg_salary'].idxmax()]['region']
                    analysis['highest_region_salary'] = float(df['avg_salary'].max())
                else:
                    analysis['highest_paid_region'] = '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö'
                    analysis['highest_region_salary'] = 0
                
                # –†–µ–≥–∏–æ–Ω—ã —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º —Å–µ–≥–º–µ–Ω—Ç–æ–≤
                analysis['most_diverse_regions'] = df.nlargest(5, 'segments_present').to_dict('records')
                
                self.logger.info(f"üåç –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(df)} —Ä–µ–≥–∏–æ–Ω–æ–≤")
                
            if self.config.cache_results:
                self._cache[cache_key] = analysis
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {e}")
            
        return analysis

    def analyze_industrial_depth(self) -> Dict[str, Any]:
        """
        –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫.
        SQLite-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è.
        """
        cache_key = "industrial_depth"
        if self.config.cache_results and cache_key in self._cache:
            return self._cache[cache_key]
            
        analysis = {}
        
        try:
            # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º —Å–µ–≥–º–µ–Ω—Ç-—É—Ä–æ–≤–µ–Ω—å
            query_combinations = """
                SELECT 
                    industry_segment,
                    position_level,
                    COUNT(*) as vacancy_count,
                    AVG(salary_avg_rub) as avg_salary,
                    COUNT(DISTINCT region) as regions_covered
                FROM vacancies 
                WHERE is_industrial = 1 
                AND industry_segment IS NOT NULL 
                AND position_level IS NOT NULL
                GROUP BY industry_segment, position_level
                HAVING vacancy_count >= 5
                ORDER BY industry_segment, position_level
            """
            
            df_combinations = pd.read_sql_query(query_combinations, self.connection)
            analysis['segment_level_combinations'] = df_combinations.to_dict('records')
            
            # –ù–∞–∏–±–æ–ª–µ–µ –≤–æ—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
            analysis['top_combinations'] = df_combinations.nlargest(10, 'vacancy_count').to_dict('records')
            
            # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            query_keywords = """
                SELECT 
                    industrial_keywords,
                    COUNT(*) as frequency
                FROM vacancies 
                WHERE is_industrial = 1 AND industrial_keywords IS NOT NULL AND industrial_keywords != ''
                GROUP BY industrial_keywords
                ORDER BY frequency DESC
                LIMIT 20
            """
            
            df_keywords = pd.read_sql_query(query_keywords, self.connection)
            analysis['industrial_keywords'] = df_keywords.to_dict('records')
            
            self.logger.info("üè≠ –ì–ª—É–±–æ–∫–∏–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
            
            if self.config.cache_results:
                self._cache[cache_key] = analysis
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–ª—É–±–∏–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            
        return analysis

    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –≤—Å–µ–º –∞—Å–ø–µ–∫—Ç–∞–º.
        JSON-—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º–∞—è –≤–µ—Ä—Å–∏—è.
        """
        self.logger.info("üìã –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
        
        report = {
            'generated_at': datetime.now().isoformat(),
            'basic_statistics': self._make_json_serializable(self.get_basic_statistics()),
            'industry_segments': self._make_json_serializable(self.analyze_industry_segments_distribution()),
            'position_levels': self._make_json_serializable(self.analyze_position_levels_distribution()),
            'salary_comparison': self._make_json_serializable(self.analyze_salary_comparison()),
            'dynamics': self._make_json_serializable(self.analyze_dynamics('monthly')),
            'skills': self._make_json_serializable(self.analyze_skills_distribution(25)),
            'regional': self._make_json_serializable(self.analyze_regional_distribution()),
            'industrial_depth': self._make_json_serializable(self.analyze_industrial_depth())
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã
        report['key_findings'] = self._extract_key_findings(report)
        
        self.logger.info("‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
        return report

    def _make_json_serializable(self, obj: Any) -> Any:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–±—ä–µ–∫—Ç –≤ JSON-—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç.
        """
        if isinstance(obj, (pd.Timestamp, datetime)):
            return obj.isoformat()
        elif isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        else:
            return obj

    def _extract_key_findings(self, report: Dict) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã –∏–∑ –æ—Ç—á–µ—Ç–∞.
        """
        findings = []
        
        try:
            basic_stats = report['basic_statistics']
            segments = report['industry_segments']
            levels = report['position_levels']
            salaries = report['salary_comparison']
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            total_vacancies = basic_stats.get('total_vacancies', 0)
            if total_vacancies >= 1000:
                findings.append(f"–°–æ–±—Ä–∞–Ω —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö: {total_vacancies:,} –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π")
            
            # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π —Å–µ–≥–º–µ–Ω—Ç
            if segments.get('dominant_segment'):
                findings.append(f"–ù–∞–∏–±–æ–ª—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –≤ —Å–µ–≥–º–µ–Ω—Ç–µ: {segments['dominant_segment']} ({segments.get('dominant_percentage', 0):.1f}%)")
            
            # –ù–∞–∏–±–æ–ª–µ–µ –≤–æ—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
            if levels.get('most_demanded_level'):
                findings.append(f"–ù–∞–∏–±–æ–ª–µ–µ –≤–æ—Å—Ç—Ä–µ–±–æ–≤–∞–Ω—ã —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã —É—Ä–æ–≤–Ω—è: {levels['most_demanded_level']}")
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞—Ä–ø–ª–∞—Ç
            if salaries.get('engineer_vs_worker'):
                ratio = salaries['engineer_vs_worker'].get('salary_ratio', 0)
                if ratio > 1:
                    findings.append(f"–ò–Ω–∂–µ–Ω–µ—Ä—ã –ø–æ–ª—É—á–∞—é—Ç –≤ {ratio:.1f} —Ä–∞–∑ –±–æ–ª—å—à–µ —Ä–∞–±–æ—á–∏—Ö")
            
            # –î–∏–Ω–∞–º–∏–∫–∞
            dynamics = report['dynamics']
            if dynamics.get('growth_rate'):
                growth = dynamics['growth_rate']
                if growth > 0:
                    findings.append(f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ —Å–ø—Ä–æ—Å–∞: +{growth:.1f}% –∑–∞ –ø–µ—Ä–∏–æ–¥")
                elif growth < 0:
                    findings.append(f"–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ —Å–ø—Ä–æ—Å–∞: {growth:.1f}% –∑–∞ –ø–µ—Ä–∏–æ–¥")
            
            # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            regional = report['regional']
            if regional.get('dominant_region'):
                findings.append(f"–õ–∏–¥–∏—Ä—É—é—â–∏–π —Ä–µ–≥–∏–æ–Ω: {regional['dominant_region']} ({regional.get('dominant_region_percentage', 0):.1f}% –≤–∞–∫–∞–Ω—Å–∏–π)")
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—ã–≤–æ–¥–æ–≤: {e}")
            
        return findings

    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        self._cache.clear()
        self.logger.info("üßπ –ö—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—á–∏—â–µ–Ω")

    def close_connection(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö."""
        if self.connection:
            self.connection.close()
            self.logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–æ")


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
def run_industrial_analysis():
    """
    –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π.
    """
    analyzer = IndustrialDataAnalyzer()
    
    if analyzer.connect_to_database():
        print("üöÄ –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê –ü–†–û–ú–´–®–õ–ï–ù–ù–´–• –í–ê–ö–ê–ù–°–ò–ô")
        print("=" * 60)
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = analyzer.get_basic_statistics()
        print(f"üìä –í—Å–µ–≥–æ –≤–∞–∫–∞–Ω—Å–∏–π: {stats.get('total_vacancies', 0):,}")
        print(f"üí∞ –°–æ –∑–∞—Ä–ø–ª–∞—Ç–æ–π: {stats.get('vacancies_with_salary', 0):,} ({stats.get('salary_coverage_percent', 0):.1f}%)")
        print(f"üè¢ –†–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–µ–π: {stats.get('unique_employers', 0):,}")
        print(f"üåç –†–µ–≥–∏–æ–Ω–æ–≤: {stats.get('unique_regions', 0):,}")
        print(f"üí∏ –°—Ä–µ–¥–Ω—è—è –∑–∞—Ä–ø–ª–∞—Ç–∞: {stats.get('avg_salary', 0):,.0f} —Ä—É–±")
        print(f"üìà –ú–µ–¥–∏–∞–Ω–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞: {stats.get('median_salary', 0):,.0f} —Ä—É–±")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        report = analyzer.generate_comprehensive_report()
        
        # –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã
        print("\nüéØ –ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´:")
        findings = report.get('key_findings', [])
        if findings:
            for i, finding in enumerate(findings[:5], 1):
                print(f"  {i}. {finding}")
        else:
            print("  ‚ÑπÔ∏è  –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
        
        analyzer.close_connection()
        
        return report
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        return None


if __name__ == "__main__":
    report = run_industrial_analysis()
    if report:
        print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {report['generated_at']}")