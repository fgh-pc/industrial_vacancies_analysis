"""
–£–¢–ò–õ–ò–¢–ê –î–õ–Ø –û–ë–™–ï–î–ò–ù–ï–ù–ò–Ø JSON –§–ê–ô–õ–û–í –ò –ì–ï–ù–ï–†–ê–¶–ò–ò –û–¢–ß–ï–¢–ê
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏, —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ–∑–¥–∞–µ—Ç –æ—Ç—á–µ—Ç
"""

import json
import os
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Set
import glob
import matplotlib.pyplot as plt
import seaborn as sns

class VacancyMerger:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è JSON —Ñ–∞–π–ª–æ–≤ —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤.
    """
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.all_vacancies = []
        self.stats = {
            'total_files_processed': 0,
            'total_vacancies_before': 0,
            'total_vacancies_after': 0,
            'duplicates_removed': 0,
            'date_range': {'min': None, 'max': None},
            'regions_count': 0,
            'industries_count': 0,
            'professional_roles_count': 0,
            'salary_stats': {},
            'collection_methods': {}
        }
        
    def find_json_files(self) -> List[str]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ JSON —Ñ–∞–π–ª—ã —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ data.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ JSON —Ñ–∞–π–ª–∞–º
        """
        pattern = os.path.join(self.data_dir, "*.json")
        json_files = glob.glob(pattern)
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ñ–∞–π–ª—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ —É–∂–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        excluded_keywords = ['stats', 'report', 'merged', 'final', 'duplicates']
        filtered_files = [
            f for f in json_files 
            if not any(keyword in f.lower() for keyword in excluded_keywords)
        ]
        
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤: {len(filtered_files)}")
        return filtered_files
    
    def load_and_merge_files(self, json_files: List[str]) -> List[Dict]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö JSON —Ñ–∞–π–ª–æ–≤.
        
        Args:
            json_files: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ JSON —Ñ–∞–π–ª–∞–º
            
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π
        """
        all_vacancies = []
        
        for file_path in json_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if isinstance(data, list):
                    all_vacancies.extend(data)
                    self.stats['total_files_processed'] += 1
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ {os.path.basename(file_path)}")
                else:
                    print(f"‚ö†Ô∏è –§–∞–π–ª {file_path} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π")
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
        
        self.stats['total_vacancies_before'] = len(all_vacancies)
        print(f"üìä –í—Å–µ–≥–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {len(all_vacancies):,}")
        
        return all_vacancies
    
    def remove_duplicates(self, vacancies: List[Dict]) -> List[Dict]:
        """
        –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ ID.
        
        Args:
            vacancies: –°–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π
            
        Returns:
            –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π
        """
        seen_ids = set()
        unique_vacancies = []
        
        for vacancy in vacancies:
            vacancy_id = vacancy.get('id')
            if vacancy_id and vacancy_id not in seen_ids:
                seen_ids.add(vacancy_id)
                unique_vacancies.append(vacancy)
        
        duplicates_removed = len(vacancies) - len(unique_vacancies)
        self.stats['duplicates_removed'] = duplicates_removed
        self.stats['total_vacancies_after'] = len(unique_vacancies)
        
        print(f"üîÑ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed:,}")
        print(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {len(unique_vacancies):,}")
        
        return unique_vacancies
    
    def analyze_vacancies(self, vacancies: List[Dict]):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
        
        Args:
            vacancies: –°–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        """
        if not vacancies:
            return
            
        # –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç
        dates = []
        for vacancy in vacancies:
            published_at = vacancy.get('published_at')
            if published_at:
                try:
                    date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                    dates.append(date)
                except:
                    continue
        
        if dates:
            self.stats['date_range']['min'] = min(dates)
            self.stats['date_range']['max'] = max(dates)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–≥–∏–æ–Ω–æ–≤
        regions = set()
        for vacancy in vacancies:
            region = vacancy.get('collection_region') or vacancy.get('area', {}).get('name')
            if region:
                regions.add(region)
        self.stats['regions_count'] = len(regions)
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞—Ä–ø–ª–∞—Ç
        salaries = []
        for vacancy in vacancies:
            salary = vacancy.get('salary')
            if salary:
                salary_from = salary.get('from')
                salary_to = salary.get('to')
                if salary_from:
                    salaries.append(salary_from)
                if salary_to:
                    salaries.append(salary_to)
        
        if salaries:
            self.stats['salary_stats'] = {
                'min': min(salaries),
                'max': max(salaries),
                'avg': sum(salaries) / len(salaries),
                'median': sorted(salaries)[len(salaries) // 2],
                'count': len(salaries)
            }
        
        # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ —Å–±–æ—Ä–∞
        collection_methods = {}
        for vacancy in vacancies:
            method = vacancy.get('collection_method', 'unknown')
            collection_methods[method] = collection_methods.get(method, 0) + 1
        self.stats['collection_methods'] = collection_methods
        
        # –ê–Ω–∞–ª–∏–∑ –æ—Ç—Ä–∞—Å–ª–µ–π –∏ —Ä–æ–ª–µ–π
        industries = set()
        professional_roles = set()
        
        for vacancy in vacancies:
            industry_id = vacancy.get('industry_id')
            if industry_id:
                industries.add(industry_id)
            
            role_id = vacancy.get('role_id')
            if role_id:
                professional_roles.add(role_id)
        
        self.stats['industries_count'] = len(industries)
        self.stats['professional_roles_count'] = len(professional_roles)
    
    def generate_report(self, output_file: str = "merged_vacancies_report.md"):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.
        
        Args:
            output_file: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—á–µ—Ç–∞
        """
        report = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á–µ—Ç–∞
        report.append("# üìä –û–¢–ß–ï–¢ –ü–û –û–ë–™–ï–î–ò–ù–ï–ù–ù–´–ú –ü–†–û–ú–´–®–õ–ï–ù–ù–´–ú –í–ê–ö–ê–ù–°–ò–Ø–ú")
        report.append(f"**–î–∞—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        report.append("## üìà –û–°–ù–û–í–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        report.append("")
        report.append(f"- **–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤:** {self.stats['total_files_processed']}")
        report.append(f"- **–í–∞–∫–∞–Ω—Å–∏–π –¥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:** {self.stats['total_vacancies_before']:,}")
        report.append(f"- **–í–∞–∫–∞–Ω—Å–∏–π –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:** {self.stats['total_vacancies_after']:,}")
        report.append(f"- **–î—É–±–ª–∏–∫–∞—Ç–æ–≤ —É–¥–∞–ª–µ–Ω–æ:** {self.stats['duplicates_removed']:,}")
        report.append(f"- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ—á–∏—Å—Ç–∫–∏:** {(self.stats['duplicates_removed'] / self.stats['total_vacancies_before'] * 100):.1f}%")
        report.append("")
        
        # –ü–µ—Ä–∏–æ–¥ —Å–±–æ—Ä–∞
        if self.stats['date_range']['min'] and self.stats['date_range']['max']:
            date_range = self.stats['date_range']
            days_diff = (date_range['max'] - date_range['min']).days
            report.append("## üìÖ –ü–ï–†–ò–û–î –°–ë–û–†–ê –î–ê–ù–ù–´–•")
            report.append("")
            report.append(f"- **–ù–∞—á–∞–ª–æ –ø–µ—Ä–∏–æ–¥–∞:** {date_range['min'].strftime('%Y-%m-%d')}")
            report.append(f"- **–ö–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞:** {date_range['max'].strftime('%Y-%m-%d')}")
            report.append(f"- **–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {days_diff} –¥–Ω–µ–π")
            report.append("")
        
        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        report.append("## üó∫Ô∏è –ì–ï–û–ì–†–ê–§–ò–ß–ï–°–ö–û–ï –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï")
        report.append("")
        report.append(f"- **–†–µ–≥–∏–æ–Ω–æ–≤ –æ—Ö–≤–∞—á–µ–Ω–æ:** {self.stats['regions_count']}")
        report.append("")
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞—Ä–ø–ª–∞—Ç
        salary_stats = self.stats['salary_stats']
        if salary_stats:
            report.append("## üí∞ –ê–ù–ê–õ–ò–ó –ó–ê–†–ü–õ–ê–¢")
            report.append("")
            report.append(f"- **–í–∞–∫–∞–Ω—Å–∏–π —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –∑–∞—Ä–ø–ª–∞—Ç–æ–π:** {salary_stats['count']:,}")
            report.append(f"- **–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞:** {salary_stats['min']:,.0f} —Ä—É–±")
            report.append(f"- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞:** {salary_stats['max']:,.0f} —Ä—É–±")
            report.append(f"- **–°—Ä–µ–¥–Ω—è—è –∑–∞—Ä–ø–ª–∞—Ç–∞:** {salary_stats['avg']:,.0f} —Ä—É–±")
            report.append(f"- **–ú–µ–¥–∏–∞–Ω–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞:** {salary_stats['median']:,.0f} —Ä—É–±")
            report.append("")
        
        # –ú–µ—Ç–æ–¥—ã —Å–±–æ—Ä–∞
        report.append("## üîß –ú–ï–¢–û–î–´ –°–ë–û–†–ê –î–ê–ù–ù–´–•")
        report.append("")
        for method, count in self.stats['collection_methods'].items():
            percentage = (count / self.stats['total_vacancies_after']) * 100
            report.append(f"- **{method}:** {count:,} –≤–∞–∫–∞–Ω—Å–∏–π ({percentage:.1f}%)")
        report.append("")
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        report.append("## üè≠ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –î–ê–ù–ù–´–•")
        report.append("")
        report.append(f"- **–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–π:** {self.stats['industries_count']}")
        report.append(f"- **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–æ–ª–µ–π:** {self.stats['professional_roles_count']}")
        report.append("")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report.append("## üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ò –í–´–í–û–î–´")
        report.append("")
        
        if self.stats['duplicates_removed'] > 0:
            report.append(f"‚úÖ **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞:** –£–¥–∞–ª–µ–Ω–æ {self.stats['duplicates_removed']:,} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        if self.stats['regions_count'] > 50:
            report.append("‚úÖ **–®–∏—Ä–æ–∫–∏–π –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –æ—Ö–≤–∞—Ç:** –î–∞–Ω–Ω—ã–µ –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤")
        else:
            report.append("‚ö†Ô∏è **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –æ—Ö–≤–∞—Ç:** –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö")
        
        if salary_stats and salary_stats['count'] / self.stats['total_vacancies_after'] > 0.5:
            report.append("‚úÖ **–•–æ—Ä–æ—à–∞—è –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –∑–∞—Ä–ø–ª–∞—Ç:** –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—Ä–ø–ª–∞—Ç–µ")
        else:
            report.append("‚ö†Ô∏è **–ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞—Ä–ø–ª–∞—Ç–∞—Ö:** –ú–Ω–æ–≥–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—Ä–ø–ª–∞—Ç–µ")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        report_path = os.path.join(self.data_dir, output_file)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        
        return report_path
    
    def create_visualizations(self, vacancies: List[Dict]):
        """
        –°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ—Ç—á–µ—Ç–∞.
        
        Args:
            vacancies: –°–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
            plots_dir = os.path.join(self.data_dir, "plots")
            os.makedirs(plots_dir, exist_ok=True)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            df_data = []
            for vacancy in vacancies:
                row = {
                    'id': vacancy.get('id'),
                    'published_at': vacancy.get('published_at'),
                    'region': vacancy.get('collection_region') or vacancy.get('area', {}).get('name'),
                    'salary_from': vacancy.get('salary', {}).get('from'),
                    'salary_to': vacancy.get('salary', {}).get('to'),
                    'collection_method': vacancy.get('collection_method', 'unknown')
                }
                df_data.append(row)
            
            df = pd.DataFrame(df_data)
            
            # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –º–µ—Å—è—Ü–∞–º
            if 'published_at' in df.columns and not df['published_at'].isna().all():
                df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce')
                df['month'] = df['published_at'].dt.to_period('M')
                
                monthly_counts = df['month'].value_counts().sort_index()
                
                plt.figure(figsize=(12, 6))
                monthly_counts.plot(kind='bar', color='skyblue')
                plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –º–µ—Å—è—Ü–∞–º')
                plt.xlabel('–ú–µ—Å—è—Ü')
                plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π')
                plt.xticks(rotation=45)
                plt.tight_layout()
                plt.savefig(os.path.join(plots_dir, 'monthly_distribution.png'), dpi=300, bbox_inches='tight')
                plt.close()
            
            # 2. –¢–æ–ø-10 —Ä–µ–≥–∏–æ–Ω–æ–≤
            if 'region' in df.columns:
                top_regions = df['region'].value_counts().head(10)
                
                plt.figure(figsize=(10, 6))
                top_regions.plot(kind='barh', color='lightgreen')
                plt.title('–¢–æ–ø-10 —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤–∞–∫–∞–Ω—Å–∏–π')
                plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π')
                plt.tight_layout()
                plt.savefig(os.path.join(plots_dir, 'top_regions.png'), dpi=300, bbox_inches='tight')
                plt.close()
            
            # 3. –ú–µ—Ç–æ–¥—ã —Å–±–æ—Ä–∞
            if 'collection_method' in df.columns:
                method_counts = df['collection_method'].value_counts()
                
                plt.figure(figsize=(8, 8))
                plt.pie(method_counts.values, labels=method_counts.index, autopct='%1.1f%%')
                plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –º–µ—Ç–æ–¥–∞–º —Å–±–æ—Ä–∞')
                plt.savefig(os.path.join(plots_dir, 'collection_methods.png'), dpi=300, bbox_inches='tight')
                plt.close()
                
            print(f"üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {plots_dir}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: {e}")
    
    def merge_and_analyze(self, output_filename: str = "merged_industrial_vacancies.json"):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞.
        
        Args:
            output_filename: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            
        Returns:
            –ü—É—Ç—å –∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        print("=" * 60)
        print("üîÑ –ù–ê–ß–ê–õ–û –û–ë–™–ï–î–ò–ù–ï–ù–ò–Ø JSON –§–ê–ô–õ–û–í")
        print("=" * 60)
        
        # –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª—ã
        json_files = self.find_json_files()
        if not json_files:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
        all_vacancies = self.load_and_merge_files(json_files)
        if not all_vacancies:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏")
            return None
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_vacancies = self.remove_duplicates(all_vacancies)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        self.analyze_vacancies(unique_vacancies)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        output_path = os.path.join(self.data_dir, output_filename)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(unique_vacancies, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        
        # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        self.create_visualizations(unique_vacancies)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report_path = self.generate_report()
        
        print("=" * 60)
        print("‚úÖ –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print("=" * 60)
        print(f"üìÅ –ò—Å—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã: {self.stats['total_files_processed']}")
        print(f"üìä –í–∞–∫–∞–Ω—Å–∏–π –¥–æ: {self.stats['total_vacancies_before']:,}")
        print(f"üìä –í–∞–∫–∞–Ω—Å–∏–π –ø–æ—Å–ª–µ: {self.stats['total_vacancies_after']:,}")
        print(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ —É–¥–∞–ª–µ–Ω–æ: {self.stats['duplicates_removed']:,}")
        print(f"üìÑ –û—Ç—á–µ—Ç: {report_path}")
        print(f"üíæ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {output_path}")
        
        return output_path


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è."""
    merger = VacancyMerger()
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        merged_file = merger.merge_and_analyze("FINAL_MERGED_INDUSTRIAL_VACANCIES.json")
        
        if merged_file:
            print(f"\nüéâ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìà –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   ‚Ä¢ –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {merger.stats['total_files_processed']}")
            print(f"   ‚Ä¢ –í–∞–∫–∞–Ω—Å–∏–π —Å–æ–±—Ä–∞–Ω–æ: {merger.stats['total_vacancies_after']:,}")
            print(f"   ‚Ä¢ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ —É–¥–∞–ª–µ–Ω–æ: {merger.stats['duplicates_removed']:,}")
            print(f"   ‚Ä¢ –†–µ–≥–∏–æ–Ω–æ–≤ –æ—Ö–≤–∞—á–µ–Ω–æ: {merger.stats['regions_count']}")
            
            if merger.stats['date_range']['min']:
                date_range = merger.stats['date_range']
                print(f"   ‚Ä¢ –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {date_range['min'].strftime('%Y-%m-%d')} - {date_range['max'].strftime('%Y-%m-%d')}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–æ–≤: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()